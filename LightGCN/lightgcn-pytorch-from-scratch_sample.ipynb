{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## In this Notebook, I have implemented Light GCN Model for Recommendation System from scratch using Pytorch.\n","metadata":{}},{"cell_type":"markdown","source":"* Light GCN was originally Developed By Microsoft and was released in Jul'2020. It is the SOTA Algorithm for Recommendation System as of now.\n* Link to the original paper -> https://paperswithcode.com/paper/lightgcn-simplifying-and-powering-graph/review/\n* There is also a Github Link to Light GCN implementation using Tensorflow -> https://github.com/microsoft/recommenders/tree/efaa3d7742183dee0846877e2dc64977098e1977\n* I have taken help from the above Repository and tried to recreate LightGCN in pytorch.\n* In the 2nd part of the notebook, I have also compared my result with the direct result from Original Tensorflow code. ","metadata":{}},{"cell_type":"markdown","source":"### Please do upvote the notebook if you liked the content. It will motivate me. Thanks !!","metadata":{}},{"cell_type":"code","source":"!pip install -q torch==1.9.0","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:28:05.433263Z","iopub.execute_input":"2021-06-27T20:28:05.433706Z","iopub.status.idle":"2021-06-27T20:29:33.951361Z","shell.execute_reply.started":"2021-06-27T20:28:05.433608Z","shell.execute_reply":"2021-06-27T20:29:33.950302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.__version__","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_colwidth', None)\nfrom sklearn import preprocessing as pp\nfrom sklearn.model_selection import train_test_split\nimport scipy.sparse as sp\nimport numpy as np\nimport random\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport time\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:31:14.585481Z","iopub.execute_input":"2021-06-27T20:31:14.58586Z","iopub.status.idle":"2021-06-27T20:31:15.570767Z","shell.execute_reply.started":"2021-06-27T20:31:14.585826Z","shell.execute_reply":"2021-06-27T20:31:15.570008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_name=['user_id','item_id','rating','timestamp']\ndf = pd.read_csv(\"../input/movielens-100k-dataset/ml-100k/u.data\",sep=\"\\t\",names=columns_name)\nprint(len(df))\ndisplay(df.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:32:14.788158Z","iopub.execute_input":"2021-06-27T20:32:14.788521Z","iopub.status.idle":"2021-06-27T20:32:14.848011Z","shell.execute_reply.started":"2021-06-27T20:32:14.788492Z","shell.execute_reply":"2021-06-27T20:32:14.84702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[df['rating']>=3]\nprint(len(df))","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:32:17.784364Z","iopub.execute_input":"2021-06-27T20:32:17.784738Z","iopub.status.idle":"2021-06-27T20:32:17.811029Z","shell.execute_reply.started":"2021-06-27T20:32:17.784707Z","shell.execute_reply":"2021-06-27T20:32:17.809022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Rating Distribution\")\ndf.groupby(['rating'])['rating'].count()","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:32:19.424266Z","iopub.execute_input":"2021-06-27T20:32:19.424662Z","iopub.status.idle":"2021-06-27T20:32:19.435417Z","shell.execute_reply.started":"2021-06-27T20:32:19.424628Z","shell.execute_reply":"2021-06-27T20:32:19.434392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = train_test_split(df.values, test_size=0.2, random_state = 16)\ntrain = pd.DataFrame(train, columns = df.columns)\ntest = pd.DataFrame(test, columns = df.columns)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:32:21.643207Z","iopub.execute_input":"2021-06-27T20:32:21.643571Z","iopub.status.idle":"2021-06-27T20:32:21.663183Z","shell.execute_reply.started":"2021-06-27T20:32:21.643539Z","shell.execute_reply":"2021-06-27T20:32:21.661686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train Size  : \", len(train))\nprint(\"Test Size : \", len (test))","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:32:23.496387Z","iopub.execute_input":"2021-06-27T20:32:23.496795Z","iopub.status.idle":"2021-06-27T20:32:23.502306Z","shell.execute_reply.started":"2021-06-27T20:32:23.496755Z","shell.execute_reply":"2021-06-27T20:32:23.501236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Label Encoding the User and Item IDs","metadata":{}},{"cell_type":"code","source":"le_user = pp.LabelEncoder()\nle_item = pp.LabelEncoder()\ntrain['user_id_idx'] = le_user.fit_transform(train['user_id'].values)\ntrain['item_id_idx'] = le_item.fit_transform(train['item_id'].values)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:32:25.164496Z","iopub.execute_input":"2021-06-27T20:32:25.165009Z","iopub.status.idle":"2021-06-27T20:32:25.189371Z","shell.execute_reply.started":"2021-06-27T20:32:25.164967Z","shell.execute_reply":"2021-06-27T20:32:25.187975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_user_ids = train['user_id'].unique()\ntrain_item_ids = train['item_id'].unique()\n\nprint(len(train_user_ids), len(train_item_ids))\n\ntest = test[(test['user_id'].isin(train_user_ids)) & (test['item_id'].isin(train_item_ids))]\nprint(len(test))","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:32:27.106924Z","iopub.execute_input":"2021-06-27T20:32:27.107309Z","iopub.status.idle":"2021-06-27T20:32:27.119161Z","shell.execute_reply.started":"2021-06-27T20:32:27.107278Z","shell.execute_reply":"2021-06-27T20:32:27.117975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['user_id_idx'] = le_user.transform(test['user_id'].values)\ntest['item_id_idx'] = le_item.transform(test['item_id'].values)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:32:32.150479Z","iopub.execute_input":"2021-06-27T20:32:32.150829Z","iopub.status.idle":"2021-06-27T20:32:32.1644Z","shell.execute_reply.started":"2021-06-27T20:32:32.150798Z","shell.execute_reply":"2021-06-27T20:32:32.163417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_users = train['user_id_idx'].nunique()\nn_items = train['item_id_idx'].nunique()\nprint(\"Number of Unique Users : \", n_users)\nprint(\"Number of unique Items : \", n_items)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:32:34.710731Z","iopub.execute_input":"2021-06-27T20:32:34.711149Z","iopub.status.idle":"2021-06-27T20:32:34.720968Z","shell.execute_reply.started":"2021-06-27T20:32:34.7111Z","shell.execute_reply":"2021-06-27T20:32:34.719748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### latent_dim is the length of the user/item embedding. \n### n_layers is the number of times we want to propagate our initial user/item embedding through the graph ","metadata":{}},{"cell_type":"code","source":"latent_dim = 64\nn_layers = 3  ","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:32:37.08587Z","iopub.execute_input":"2021-06-27T20:32:37.08623Z","iopub.status.idle":"2021-06-27T20:32:37.0913Z","shell.execute_reply.started":"2021-06-27T20:32:37.0862Z","shell.execute_reply":"2021-06-27T20:32:37.090083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_sparse_tensor(dok_mtrx):\n    \n    dok_mtrx_coo = dok_mtrx.tocoo().astype(np.float32)\n    values = dok_mtrx_coo.data\n    indices = np.vstack((dok_mtrx_coo.row, dok_mtrx_coo.col))\n\n    i = torch.LongTensor(indices)\n    v = torch.FloatTensor(values)\n    shape = dok_mtrx_coo.shape\n\n    dok_mtrx_sparse_tensor = torch.sparse.FloatTensor(i, v, torch.Size(shape))\n\n    return dok_mtrx_sparse_tensor","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:32:38.209294Z","iopub.execute_input":"2021-06-27T20:32:38.209668Z","iopub.status.idle":"2021-06-27T20:32:38.216375Z","shell.execute_reply.started":"2021-06-27T20:32:38.209632Z","shell.execute_reply":"2021-06-27T20:32:38.215438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Metrics**\n### Below Function gets the 4 different metrics out of Test Data -> Recall@K, Precision@K, NDCG@K, MAP@K where K is the top K items we would like to recommend to User. ","metadata":{}},{"cell_type":"code","source":"def get_metrics(user_Embed_wts, item_Embed_wts, n_users, n_items, train_data, test_data, K):\n\n    user_Embedding = nn.Embedding(user_Embed_wts.size()[0], user_Embed_wts.size()[1], _weight = user_Embed_wts)\n    item_Embedding = nn.Embedding(item_Embed_wts.size()[0], item_Embed_wts.size()[1], _weight = item_Embed_wts)\n\n    test_user_ids = torch.LongTensor(test_data['user_id_idx'].unique())\n\n    relevance_score = torch.matmul(user_Embed_wts, torch.transpose(item_Embed_wts,0, 1))\n\n    R = sp.dok_matrix((n_users, n_items), dtype = np.float32)\n    R[train_data['user_id_idx'], train_data['item_id_idx']] = 1.0\n\n    R_tensor = convert_to_sparse_tensor(R)\n    R_tensor_dense = R_tensor.to_dense()\n\n    R_tensor_dense = R_tensor_dense*(-np.inf)\n    R_tensor_dense = torch.nan_to_num(R_tensor_dense, nan=0.0)\n\n    relevance_score = relevance_score+R_tensor_dense\n\n    topk_relevance_score = torch.topk(relevance_score, K).values\n    topk_relevance_indices = torch.topk(relevance_score, K).indices\n\n    topk_relevance_indices_df = pd.DataFrame(topk_relevance_indices.numpy(),columns =['top_indx_'+str(x+1) for x in range(K)])\n\n    topk_relevance_indices_df['user_ID'] = topk_relevance_indices_df.index\n \n    topk_relevance_indices_df['top_rlvnt_itm'] = topk_relevance_indices_df[['top_indx_'+str(x+1) for x in range(K)]].values.tolist()\n    topk_relevance_indices_df = topk_relevance_indices_df[['user_ID','top_rlvnt_itm']]\n\n    test_interacted_items = test_data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n\n    metrics_df = pd.merge(test_interacted_items,topk_relevance_indices_df, how= 'left', left_on = 'user_id_idx',right_on = ['user_ID'])\n    metrics_df['intrsctn_itm'] = [list(set(a).intersection(b)) for a, b in zip(metrics_df.item_id_idx, metrics_df.top_rlvnt_itm)]\n\n\n    metrics_df['recall'] = metrics_df.apply(lambda x : len(x['intrsctn_itm'])/len(x['item_id_idx']), axis = 1) \n    metrics_df['precision'] = metrics_df.apply(lambda x : len(x['intrsctn_itm'])/K, axis = 1)\n\n    def get_hit_list(item_id_idx, top_rlvnt_itm):\n        return [1 if x in set(item_id_idx) else 0 for x in top_rlvnt_itm ]\n\n    metrics_df['hit_list'] = metrics_df.apply(lambda x : get_hit_list(x['item_id_idx'], x['top_rlvnt_itm']), axis = 1)\n\n    def get_dcg_idcg(item_id_idx, hit_list):\n        idcg  = sum([1 / np.log1p(idx+1) for idx in range(min(len(item_id_idx),len(hit_list)))])\n        dcg =  sum([hit / np.log1p(idx+1) for idx, hit in enumerate(hit_list)])\n        return dcg/idcg\n\n    def get_cumsum(hit_list):\n        return np.cumsum(hit_list)\n\n    def get_map(item_id_idx, hit_list, hit_list_cumsum):\n        return sum([hit_cumsum*hit/(idx+1) for idx, (hit, hit_cumsum) in enumerate(zip(hit_list, hit_list_cumsum))])/len(item_id_idx)\n\n    metrics_df['ndcg'] = metrics_df.apply(lambda x : get_dcg_idcg(x['item_id_idx'], x['hit_list']), axis = 1)\n    metrics_df['hit_list_cumsum'] = metrics_df.apply(lambda x : get_cumsum(x['hit_list']), axis = 1)\n\n    metrics_df['map'] = metrics_df.apply(lambda x : get_map(x['item_id_idx'], x['hit_list'], x['hit_list_cumsum']), axis = 1)\n\n    return metrics_df['recall'].mean(), metrics_df['precision'].mean(), metrics_df['ndcg'].mean(), metrics_df['map'].mean() ","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:33:16.076557Z","iopub.execute_input":"2021-06-27T20:33:16.076965Z","iopub.status.idle":"2021-06-27T20:33:16.100902Z","shell.execute_reply.started":"2021-06-27T20:33:16.076926Z","shell.execute_reply":"2021-06-27T20:33:16.099667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  **LightGCN Model**\n### **get_A_tilda** function is used to get A_tilda which will be multiplied with initial user/item embedding (E0) to create embedding at different layers (n_layers = 3) in **propagate_through_layers** function\n### **forward** function is used to look up for initial(E0) and final embedding of a user/item","metadata":{}},{"cell_type":"code","source":"class LightGCN(nn.Module):\n    def __init__(self, data, n_users, n_items, n_layers, latent_dim):\n        super(LightGCN, self).__init__()\n        self.data = data\n        self.n_users = n_users\n        self.n_items = n_items\n        self.n_layers = n_layers\n        self.latent_dim = latent_dim\n        self.init_embedding()\n        self.norm_adj_mat_sparse_tensor = self.get_A_tilda()\n\n    def init_embedding(self):\n        self.E0 = nn.Embedding(self.n_users + self.n_items, self.latent_dim)\n        nn.init.xavier_uniform_(self.E0.weight)\n        self.E0.weight = nn.Parameter(self.E0.weight)\n\n    def get_A_tilda(self):\n        R = sp.dok_matrix((self.n_users, self.n_items), dtype = np.float32)\n        R[self.data['user_id_idx'], self.data['item_id_idx']] = 1.0\n\n        adj_mat = sp.dok_matrix(\n                (self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32\n            )\n        adj_mat = adj_mat.tolil()\n        R = R.tolil()\n\n        adj_mat[: n_users, n_users :] = R\n        adj_mat[n_users :, : n_users] = R.T\n        adj_mat = adj_mat.todok()\n\n        rowsum = np.array(adj_mat.sum(1))\n        d_inv = np.power(rowsum + 1e-9, -0.5).flatten()\n        d_inv[np.isinf(d_inv)] = 0.0\n        d_mat_inv = sp.diags(d_inv)\n        norm_adj_mat = d_mat_inv.dot(adj_mat)\n        norm_adj_mat = norm_adj_mat.dot(d_mat_inv)\n        \n        # Below Code is toconvert the dok_matrix to sparse tensor.\n        \n        norm_adj_mat_coo = norm_adj_mat.tocoo().astype(np.float32)\n        values = norm_adj_mat_coo.data\n        indices = np.vstack((norm_adj_mat_coo.row, norm_adj_mat_coo.col))\n\n        i = torch.LongTensor(indices)\n        v = torch.FloatTensor(values)\n        shape = norm_adj_mat_coo.shape\n\n        norm_adj_mat_sparse_tensor = torch.sparse.FloatTensor(i, v, torch.Size(shape))\n\n        return norm_adj_mat_sparse_tensor\n    \n    def propagate_through_layers(self):\n        all_layer_embedding = [self.E0.weight]\n        E_lyr = self.E0.weight\n\n        for layer in range(self.n_layers):\n            E_lyr = torch.sparse.mm(self.norm_adj_mat_sparse_tensor, E_lyr)\n            all_layer_embedding.append(E_lyr)\n\n        all_layer_embedding = torch.stack(all_layer_embedding)\n        mean_layer_embedding = torch.mean(all_layer_embedding, axis = 0)\n\n        final_user_Embed, final_item_Embed = torch.split(mean_layer_embedding, [n_users, n_items])\n        initial_user_Embed, initial_item_Embed = torch.split(self.E0.weight, [n_users, n_items])\n\n        return final_user_Embed, final_item_Embed, initial_user_Embed, initial_item_Embed\n\n    def forward(self, users, pos_items, neg_items):\n        final_user_Embed, final_item_Embed, initial_user_Embed, initial_item_Embed = self.propagate_through_layers()\n\n        users_emb, pos_emb, neg_emb = final_user_Embed[users], final_item_Embed[pos_items], final_item_Embed[neg_items]\n        userEmb0,  posEmb0, negEmb0 = initial_user_Embed[users], initial_item_Embed[pos_items], initial_item_Embed[neg_items]\n\n        return users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:33:31.37398Z","iopub.execute_input":"2021-06-27T20:33:31.374368Z","iopub.status.idle":"2021-06-27T20:33:31.393303Z","shell.execute_reply.started":"2021-06-27T20:33:31.374336Z","shell.execute_reply":"2021-06-27T20:33:31.392321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lightGCN = LightGCN(train, n_users, n_items, n_layers, latent_dim)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:33:35.180413Z","iopub.execute_input":"2021-06-27T20:33:35.18079Z","iopub.status.idle":"2021-06-27T20:33:35.941784Z","shell.execute_reply.started":"2021-06-27T20:33:35.180754Z","shell.execute_reply":"2021-06-27T20:33:35.940722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Size of Learnable Embedding : \", list(lightGCN.parameters())[0].size())","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:33:38.129087Z","iopub.execute_input":"2021-06-27T20:33:38.129486Z","iopub.status.idle":"2021-06-27T20:33:38.134574Z","shell.execute_reply.started":"2021-06-27T20:33:38.129443Z","shell.execute_reply":"2021-06-27T20:33:38.133797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **BPR Loss**","metadata":{}},{"cell_type":"code","source":"def bpr_loss(users, users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0):\n  \n    reg_loss = (1/2)*(userEmb0.norm().pow(2) + \n                    posEmb0.norm().pow(2)  +\n                    negEmb0.norm().pow(2))/float(len(users))\n    pos_scores = torch.mul(users_emb, pos_emb)\n    pos_scores = torch.sum(pos_scores, dim=1)\n    neg_scores = torch.mul(users_emb, neg_emb)\n    neg_scores = torch.sum(neg_scores, dim=1)\n        \n    loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n        \n    return loss, reg_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:33:40.343017Z","iopub.execute_input":"2021-06-27T20:33:40.343368Z","iopub.status.idle":"2021-06-27T20:33:40.35079Z","shell.execute_reply.started":"2021-06-27T20:33:40.34334Z","shell.execute_reply":"2021-06-27T20:33:40.349334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Loader - Samples users and for each user it sample 1 postive item - which User interacted with in Training Data and 1 negative item - with which User have not interacted.","metadata":{}},{"cell_type":"code","source":"def data_loader(data, batch_size, n_usr, n_itm):\n  \n    interected_items_df = data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n  \n    def sample_neg(x):\n        while True:\n            neg_id = random.randint(0, n_itm - 1)\n            if neg_id not in x:\n                return neg_id\n  \n    indices = [x for x in range(n_usr)]\n    \n    if n_usr < batch_size:\n        users = [random.choice(indices) for _ in range(batch_size)]\n    else:\n        users = random.sample(indices, batch_size)\n\n    users.sort()\n  \n    users_df = pd.DataFrame(users,columns = ['users'])\n\n    interected_items_df = pd.merge(interected_items_df, users_df, how = 'right', left_on = 'user_id_idx', right_on = 'users')\n  \n    pos_items = interected_items_df['item_id_idx'].apply(lambda x : random.choice(x)).values\n\n    neg_items = interected_items_df['item_id_idx'].apply(lambda x: sample_neg(x)).values\n\n    return list(users), list(pos_items), list(neg_items)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:34:04.59682Z","iopub.execute_input":"2021-06-27T20:34:04.597209Z","iopub.status.idle":"2021-06-27T20:34:04.607266Z","shell.execute_reply.started":"2021-06-27T20:34:04.597173Z","shell.execute_reply":"2021-06-27T20:34:04.606173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(lightGCN.parameters(), lr = 0.005)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:33:44.244553Z","iopub.execute_input":"2021-06-27T20:33:44.2449Z","iopub.status.idle":"2021-06-27T20:33:44.249952Z","shell.execute_reply.started":"2021-06-27T20:33:44.244871Z","shell.execute_reply":"2021-06-27T20:33:44.248661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 30\nBATCH_SIZE = 1024 \nDECAY = 0.0001\nK = 10","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:33:49.698033Z","iopub.execute_input":"2021-06-27T20:33:49.698387Z","iopub.status.idle":"2021-06-27T20:33:49.702649Z","shell.execute_reply.started":"2021-06-27T20:33:49.698359Z","shell.execute_reply":"2021-06-27T20:33:49.701758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Training Loop**","metadata":{}},{"cell_type":"code","source":"loss_list_epoch = []\nMF_loss_list_epoch = []\nreg_loss_list_epoch = []\n\nrecall_list = []\nprecision_list = []\nndcg_list = []\nmap_list = []\n\ntrain_time_list = []\neval_time_list = [] \n\nfor epoch in tqdm(range(EPOCHS)):\n    n_batch = int(len(train)/BATCH_SIZE)\n  \n    final_loss_list = []\n    MF_loss_list = []\n    reg_loss_list = []\n  \n    best_ndcg = -1\n  \n    train_start_time = time.time()\n    lightGCN.train()\n    for batch_idx in range(n_batch):\n\n        optimizer.zero_grad()\n\n        users, pos_items, neg_items = data_loader(train, BATCH_SIZE, n_users, n_items)\n\n        users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0 = lightGCN.forward(users, pos_items, neg_items)\n\n        mf_loss, reg_loss = bpr_loss(users, users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0)\n        reg_loss = DECAY * reg_loss\n        final_loss = mf_loss + reg_loss\n\n        final_loss.backward()\n        optimizer.step()\n\n        final_loss_list.append(final_loss.item())\n        MF_loss_list.append(mf_loss.item())\n        reg_loss_list.append(reg_loss.item())\n\n\n    train_end_time = time.time()\n    train_time = train_end_time - train_start_time\n\n    lightGCN.eval()\n    with torch.no_grad():\n    \n        final_user_Embed, final_item_Embed, initial_user_Embed,initial_item_Embed = lightGCN.propagate_through_layers()\n        test_topK_recall,  test_topK_precision, test_topK_ndcg, test_topK_map  = get_metrics(final_user_Embed, final_item_Embed, n_users, n_items, train, test, K)\n\n\n    if test_topK_ndcg > best_ndcg:\n        best_ndcg = test_topK_ndcg\n      \n        torch.save(final_user_Embed, 'final_user_Embed.pt')\n        torch.save(final_item_Embed, 'final_item_Embed.pt')\n        torch.save(initial_user_Embed, 'initial_user_Embed.pt')\n        torch.save(initial_item_Embed, 'initial_item_Embed.pt')\n     \n\n    eval_time = time.time() - train_end_time\n\n    loss_list_epoch.append(round(np.mean(final_loss_list),4))\n    MF_loss_list_epoch.append(round(np.mean(MF_loss_list),4))\n    reg_loss_list_epoch.append(round(np.mean(reg_loss_list),4))\n\n    recall_list.append(round(test_topK_recall,4))\n    precision_list.append(round(test_topK_precision,4))\n    ndcg_list.append(round(test_topK_ndcg,4))\n    map_list.append(round(test_topK_map,4))\n\n    train_time_list.append(train_time)\n    eval_time_list.append(eval_time)  ","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:34:12.936367Z","iopub.execute_input":"2021-06-27T20:34:12.936717Z","iopub.status.idle":"2021-06-27T20:38:48.98772Z","shell.execute_reply.started":"2021-06-27T20:34:12.936689Z","shell.execute_reply":"2021-06-27T20:38:48.986382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch_list = [(i+1) for i in range(EPOCHS)]","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:39:10.394779Z","iopub.execute_input":"2021-06-27T20:39:10.395209Z","iopub.status.idle":"2021-06-27T20:39:10.400446Z","shell.execute_reply.started":"2021-06-27T20:39:10.395175Z","shell.execute_reply":"2021-06-27T20:39:10.399437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(epoch_list, recall_list, label='Recall')\nplt.plot(epoch_list, precision_list, label='Precision')\nplt.plot(epoch_list, ndcg_list, label='NDCG')\nplt.plot(epoch_list, map_list, label='MAP')\nplt.xlabel('Epoch')\nplt.ylabel('Metrics')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:39:12.438542Z","iopub.execute_input":"2021-06-27T20:39:12.438887Z","iopub.status.idle":"2021-06-27T20:39:12.715678Z","shell.execute_reply.started":"2021-06-27T20:39:12.438858Z","shell.execute_reply":"2021-06-27T20:39:12.714486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(epoch_list, loss_list_epoch, label='Total Training Loss')\nplt.plot(epoch_list, MF_loss_list_epoch, label='MF Training Loss')\nplt.plot(epoch_list, reg_loss_list_epoch, label='Reg Training Loss')\n\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:39:26.415329Z","iopub.execute_input":"2021-06-27T20:39:26.415926Z","iopub.status.idle":"2021-06-27T20:39:26.641685Z","shell.execute_reply.started":"2021-06-27T20:39:26.415884Z","shell.execute_reply":"2021-06-27T20:39:26.640497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Averge time taken to train an epoch -> \", round(np.mean(train_time_list),2), \" seconds\")\nprint(\"Averge time taken to eval an epoch -> \", round(np.mean(eval_time_list),2), \" seconds\")","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:39:35.77767Z","iopub.execute_input":"2021-06-27T20:39:35.778091Z","iopub.status.idle":"2021-06-27T20:39:35.785068Z","shell.execute_reply.started":"2021-06-27T20:39:35.778051Z","shell.execute_reply":"2021-06-27T20:39:35.784097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Last Epoch's Test Data Recall -> \", recall_list[-1])\nprint(\"Last Epoch's Test Data Precision -> \", precision_list[-1])\nprint(\"Last Epoch's Test Data NDCG -> \", ndcg_list[-1])\nprint(\"Last Epoch's Test Data MAP -> \", map_list[-1])\n\nprint(\"Last Epoch's Train Data Loss -> \", loss_list_epoch[-1])","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:40:11.483313Z","iopub.execute_input":"2021-06-27T20:40:11.483701Z","iopub.status.idle":"2021-06-27T20:40:11.491432Z","shell.execute_reply.started":"2021-06-27T20:40:11.483668Z","shell.execute_reply":"2021-06-27T20:40:11.489958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## In the below part, I have used LightGCN original Code from Microsoft's Repository, generated the Metrics, Losses and compared that with my own output.\n#### Steps to run original LightGCN in tensorflow -> https://github.com/microsoft/recommenders/blob/main/examples/07_tutorials/KDD2020-tutorial/step5_run_lightgcn.ipynb","metadata":{}},{"cell_type":"code","source":"!pip install -q tensorflow==1.15 ","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:40:30.943353Z","iopub.execute_input":"2021-06-27T20:40:30.94408Z","iopub.status.idle":"2021-06-27T20:41:50.934536Z","shell.execute_reply.started":"2021-06-27T20:40:30.944038Z","shell.execute_reply":"2021-06-27T20:41:50.93249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/microsoft/recommenders.git ./recommenders_microsoft","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:50:53.785305Z","iopub.execute_input":"2021-06-27T20:50:53.785813Z","iopub.status.idle":"2021-06-27T20:51:05.908036Z","shell.execute_reply.started":"2021-06-27T20:50:53.785773Z","shell.execute_reply":"2021-06-27T20:51:05.906666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport os\nsys.path.insert(0, './recommenders_microsoft')\nsys.path.insert(0, './recommenders_microsoft/reco_utils')","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:52:22.878156Z","iopub.execute_input":"2021-06-27T20:52:22.878538Z","iopub.status.idle":"2021-06-27T20:52:22.883806Z","shell.execute_reply.started":"2021-06-27T20:52:22.878504Z","shell.execute_reply":"2021-06-27T20:52:22.882903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom reco_utils.common.timer import Timer\nfrom reco_utils.recommender.deeprec.models.graphrec.lightgcn import LightGCN\nfrom reco_utils.recommender.deeprec.DataModel.ImplicitCF import ImplicitCF\nfrom reco_utils.dataset import movielens\nfrom reco_utils.dataset.python_splitters import python_stratified_split\nfrom reco_utils.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\nfrom reco_utils.common.constants import SEED as DEFAULT_SEED\nfrom reco_utils.recommender.deeprec.deeprec_utils import prepare_hparams\nfrom reco_utils.recommender.deeprec.deeprec_utils import cal_metric","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:55:17.804722Z","iopub.execute_input":"2021-06-27T20:55:17.806674Z","iopub.status.idle":"2021-06-27T20:55:21.387443Z","shell.execute_reply.started":"2021-06-27T20:55:17.806621Z","shell.execute_reply":"2021-06-27T20:55:21.38633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.rename(columns = {'user_id':'userID', 'item_id':'itemID'})\ntest = test.rename(columns = {'user_id':'userID', 'item_id':'itemID'})","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:55:41.597917Z","iopub.execute_input":"2021-06-27T20:55:41.598464Z","iopub.status.idle":"2021-06-27T20:55:41.6117Z","shell.execute_reply.started":"2021-06-27T20:55:41.59843Z","shell.execute_reply":"2021-06-27T20:55:41.610397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = ImplicitCF(\n    train=train, test=test, seed=0,\n    col_user='userID',\n    col_item='itemID',\n    col_rating='rating'\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:55:48.011588Z","iopub.execute_input":"2021-06-27T20:55:48.011955Z","iopub.status.idle":"2021-06-27T20:55:48.231514Z","shell.execute_reply.started":"2021-06-27T20:55:48.011924Z","shell.execute_reply":"2021-06-27T20:55:48.230284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yaml_file = './recommenders_microsoft/examples/07_tutorials/KDD2020-tutorial/lightgcn.yaml'\n\n\nhparams = prepare_hparams(yaml_file,                          \n                          learning_rate=0.005,\n                          eval_epoch=1,\n                          top_k=10,\n                          save_model=False,\n                          epochs=30,\n                          save_epoch=1\n                         )","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:56:12.275534Z","iopub.execute_input":"2021-06-27T20:56:12.27595Z","iopub.status.idle":"2021-06-27T20:56:13.273654Z","shell.execute_reply.started":"2021-06-27T20:56:12.275909Z","shell.execute_reply":"2021-06-27T20:56:13.27271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LightGCN(hparams, data, seed=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:56:21.297183Z","iopub.execute_input":"2021-06-27T20:56:21.297555Z","iopub.status.idle":"2021-06-27T20:56:22.558596Z","shell.execute_reply.started":"2021-06-27T20:56:21.297524Z","shell.execute_reply":"2021-06-27T20:56:22.557373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with Timer() as train_time:\n    model.fit()\n\nprint(\"Took {} seconds for training.\".format(train_time.interval))","metadata":{"execution":{"iopub.status.busy":"2021-06-27T20:56:31.357102Z","iopub.execute_input":"2021-06-27T20:56:31.3575Z","iopub.status.idle":"2021-06-27T20:57:47.187156Z","shell.execute_reply.started":"2021-06-27T20:56:31.357469Z","shell.execute_reply":"2021-06-27T20:57:47.185932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As can be seen, Using both my recreated Code (in Pytorch) and original Tensorflow Implementation of Light GCN, results are quite similar. \n### At end of 30th Epoch , Recall@10 -> ~0.21, Precision@10 -> ~0.28, NDCG@10 -> ~0.35, MAP@10 -> ~0.12 and Training Loss -> ~0.12\n### Time Taken to train 1 epoch for original code is ~2.5s while through my code its ~8s and evaluation time is 0.2s and 0.4s respectively.","metadata":{}},{"cell_type":"markdown","source":"## Please upvote the notebook if you have learned from it. Thanks !!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}