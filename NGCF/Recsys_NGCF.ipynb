{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tm6qpMgBo4iL"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import multiprocessing\n",
    "from worker import worker\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLUDbHD6tUIA"
   },
   "source": [
    "## 1. 데이터 가공\n",
    "userId_problemId.csv 파일 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9S15_wWOqlrO",
    "outputId": "6e7b2600-98e0-4cf7-b0e5-803030e86a6a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-0645934d-789f-46f8-a3ab-e9cfc5701bfc\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>problemId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sos0911</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sos0911</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sos0911</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sos0911</td>\n",
       "      <td>1003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sos0911</td>\n",
       "      <td>1005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0645934d-789f-46f8-a3ab-e9cfc5701bfc')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-0645934d-789f-46f8-a3ab-e9cfc5701bfc button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-0645934d-789f-46f8-a3ab-e9cfc5701bfc');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "    userId  problemId\n",
       "0  sos0911       1000\n",
       "1  sos0911       1001\n",
       "2  sos0911       1002\n",
       "3  sos0911       1003\n",
       "4  sos0911       1005"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/userId_problemId.csv\").loc[:,['userId', 'problemId']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDzVw2frtXET"
   },
   "source": [
    "### 1) userId remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RtTOGWMUtQx4"
   },
   "outputs": [],
   "source": [
    "def remap_user():\n",
    "    user_problem_df = pd.read_csv(\"./data/userId_problemId.csv\").loc[:, ['userId', 'problemId']]\n",
    "    user_df = pd.DataFrame({\"user_id\": [], \"remap_id\": []}).astype({'remap_id':'int'})\n",
    "    unique_user = user_problem_df['userId'].unique().tolist()\n",
    "    for i in range(len(unique_user)):\n",
    "        new_user_df = pd.DataFrame({\"user_id\": [unique_user[i]], \"remap_id\": [i]})\n",
    "        user_df = pd.concat((user_df, new_user_df))\n",
    "        \n",
    "    user_df.to_csv(\"./data/user_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNoHgbOBtY6l"
   },
   "source": [
    "### 2) problemId remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kO3kEm6-tYTf"
   },
   "outputs": [],
   "source": [
    "def remap_problem():\n",
    "    user_problem_df = pd.read_csv(\"./data/userId_problemId.csv\").loc[:, ['userId', 'problemId']]\n",
    "    problem_df = pd.DataFrame({\"problem_id\": [], \"remap_id\": []}).astype({'remap_id':'int'})\n",
    "    unique_sorted_problem = sorted(user_problem_df['problemId'].unique().tolist())\n",
    "    for i in range(len(unique_sorted_problem)):\n",
    "        new_problem_df = pd.DataFrame({\"problem_id\": [unique_sorted_problem[i]], \"remap_id\": [i]})\n",
    "        problem_df = pd.concat((problem_df, new_problem_df))\n",
    "    \n",
    "    problem_df.to_csv(\"./data/problem_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvw2dsc6tcqQ"
   },
   "source": [
    "### 3) Convert userId, problemId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oK5oz3matcO4"
   },
   "outputs": [],
   "source": [
    "def remap_user_problem():\n",
    "    user_problem_df = pd.read_csv(\"./data/userId_problemId.csv\").loc[:, ['userId', 'problemId']]\n",
    "    user_df = pd.read_csv(\"./data/user_list.csv\")\n",
    "    problem_df = pd.read_csv(\"./data/problem_list.csv\")\n",
    "\n",
    "    manager = multiprocessing.Manager()\n",
    "    user_problem_remap_df = pd.DataFrame({'userId': [], 'problemId': []}).astype('int')\n",
    "\n",
    "    for i in range(0, user_problem_df['userId'].size, 10000):\n",
    "        clear_output(wait=True)\n",
    "        print('Loading: [{}]'.format('-' * (i // 10000) + '>' + '-' * (83 - i // 10000)))\n",
    "        \n",
    "        return_dict = manager.dict()\n",
    "        jobs = []\n",
    "        \n",
    "        for j in range(4):\n",
    "            p = multiprocessing.Process(target=worker, args=(j, i + 2500 * j, user_df, problem_df, user_problem_df[i + 2500 * j:i + min(user_problem_df['userId'].size, 2500*(j+1))], return_dict))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "            if i + 2500 * (j+1) >= user_problem_df['userId'].size: break\n",
    "        for proc in jobs:\n",
    "            proc.join()\n",
    "            proc.close()\n",
    "        for j in range(len(return_dict.keys())):\n",
    "            user_problem_remap_df = pd.concat((user_problem_remap_df, return_dict[j]))\n",
    "    user_problem_remap_df.to_csv(\"./data/userId_problemId_remap.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vytaxrOotiJe",
    "outputId": "8174f06b-6917-45a6-bbf4-58a7cbac09d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_list.csv file already exist\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('./data/user_list.csv'):\n",
    "    print('user_list.csv file already exist')\n",
    "else: \n",
    "    remap_user()\n",
    "    print('user_list file saved successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ieHaZKROtlTm",
    "outputId": "fe9ef85b-4be2-4dbe-994d-0f9581217d06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem_list.csv file already exist\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('./data/problem_list.csv'):\n",
    "    print('problem_list.csv file already exist')\n",
    "else:\n",
    "    remap_problem()\n",
    "    print('problem_list file saved successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKCMiJsAtq6o"
   },
   "source": [
    "multiprocessing을 사용하여 30분 이상 걸리던 작업을 13분으로 단축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VPGmdhM0tm-k",
    "outputId": "f3687775-dd64-4f5f-c340-7e107a458007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId_problemId_remap.csv file already exist\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('./data/userId_problemId_remap.csv'):\n",
    "    print('userId_problemId_remap.csv file already exist')\n",
    "else:\n",
    "    remap_user_problem()\n",
    "    print('userId_problemId_remap file saved successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxQdJFTkttde"
   },
   "source": [
    "### train data, test data 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5QHgf8I2tpCK"
   },
   "outputs": [],
   "source": [
    "def making_data():\n",
    "\n",
    "    if os.path.isfile('./data/train.txt') and os.path.isfile('./data/test.txt'):\n",
    "        print('train.txt test.txt file already exist')\n",
    "        return\n",
    "\n",
    "    user_problem_remap_df = pd.read_csv(\"./data/userId_problemId_remap.csv\").loc[:, ['userId', 'problemId']]\n",
    "    train_data_df = pd.DataFrame({'userId': [], 'problemId': []}, dtype=int)\n",
    "    test_data_df = pd.DataFrame({'userId': [], 'problemId': []}, dtype=int)\n",
    "    \n",
    "    for user_id in user_problem_remap_df['userId'].unique():\n",
    "        problem_list = user_problem_remap_df[user_problem_remap_df['userId'] == user_id]['problemId'].tolist()\n",
    "        train_problem_list = random.sample(problem_list, int(len(problem_list) * 0.8))\n",
    "        test_problem_list = list(set(problem_list) - set(train_problem_list))\n",
    "        \n",
    "        new_train_data_df = pd.DataFrame({'userId': [user_id], 'problemId': [train_problem_list]})\n",
    "        new_test_data_df = pd.DataFrame({'userId': [user_id], 'problemId': [test_problem_list]})\n",
    "        \n",
    "        train_data_df = pd.concat((train_data_df, new_train_data_df))\n",
    "        test_data_df = pd.concat((test_data_df, new_test_data_df))\n",
    "    \n",
    "    with open(\"./data/train.txt\", \"w\") as f:\n",
    "        for user_id in sorted(train_data_df['userId'].tolist()):\n",
    "            problem_list = [*map(int, train_data_df[train_data_df['userId'] == user_id]['problemId'].tolist()[0])]\n",
    "            f.write(str(user_id) + ' ' + ' '.join(map(str, problem_list)) + '\\n')\n",
    "    \n",
    "    with open(\"./data/test.txt\", \"w\") as f:\n",
    "        for user_id in sorted(test_data_df['userId'].tolist()):\n",
    "            problem_list = [*map(int, test_data_df[test_data_df['userId'] == user_id]['problemId'].tolist()[0])]\n",
    "            f.write(str(user_id) + ' ' + ' '.join(map(str, problem_list)) + '\\n')\n",
    "        \n",
    "    print('train.txt, test.txt saved successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Dx9jR-0tvO7",
    "outputId": "fe7c3588-4486-42fa-c768-6fefe7a473a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.txt test.txt file already exist\n"
     ]
    }
   ],
   "source": [
    "making_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lI7AIfcCtxbK"
   },
   "source": [
    "# NGCF 모델 관련 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2Xq4bIRtyFx"
   },
   "source": [
    "### 전역변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ENuY5LnMtwJr"
   },
   "outputs": [],
   "source": [
    "train_items, test_set = {}, {}\n",
    "matrix = None\n",
    "exist_users = []\n",
    "global_epoch_value = 0\n",
    "result_arr = []\n",
    "\n",
    "n_users, n_items, n_train, n_test = 0, 0, 0, 0\n",
    "total_epoch = 2000\n",
    "embed_size = 64\n",
    "batch_size = 1024\n",
    "layer_size = [64, 64, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z03bCWbit0wb"
   },
   "source": [
    "### NGCF 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "F_TVKh_xtz0t"
   },
   "outputs": [],
   "source": [
    "class NGCF(nn.Module):\n",
    "    def __init__(self, n_user, n_item, norm_adj, emb_size, batch_size, layer_size):\n",
    "        super(NGCF, self).__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.norm_adj = norm_adj.cuda()\n",
    "        self.emb_size = emb_size\n",
    "        self.batch_size = batch_size\n",
    "        self.layer_size = layer_size\n",
    "\n",
    "        self.embedding_dict, self.weight_dict = self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        embedding_dict = nn.ParameterDict({\n",
    "            'user_emb': nn.Parameter(nn.init.xavier_uniform_(torch.empty(self.n_user, self.emb_size))),\n",
    "            'item_emb': nn.Parameter(nn.init.xavier_uniform_(torch.empty(self.n_item, self.emb_size)))\n",
    "        })\n",
    "\n",
    "        weight_dict = nn.ParameterDict()\n",
    "        layers = [self.emb_size] + self.layer_size\n",
    "        for i in range(len(self.layer_size)):\n",
    "            weight_dict['W_gc_%d' % i] = nn.Parameter(nn.init.xavier_uniform_(torch.empty(layers[i], layers[i + 1])))\n",
    "            weight_dict['W_bi_%d' % i] = nn.Parameter(nn.init.xavier_uniform_(torch.empty(layers[i], layers[i + 1])))\n",
    "\n",
    "        return embedding_dict, weight_dict\n",
    "\n",
    "    def rating(self, u_g_embeddings, pos_i_g_embeddings):\n",
    "        return torch.matmul(u_g_embeddings, pos_i_g_embeddings.t())\n",
    "\n",
    "    def forward(self, users, pos_items, neg_items):\n",
    "        ego_embeddings = torch.cat([self.embedding_dict['user_emb'], self.embedding_dict['item_emb']], 0)\n",
    "        all_embeddings = [ego_embeddings]\n",
    "\n",
    "        for i in range(len(self.layer_size)):\n",
    "            side_embeddings = torch.mm(self.norm_adj, ego_embeddings)\n",
    "\n",
    "            sum_embeddings = torch.matmul(ego_embeddings, self.weight_dict['W_gc_%d' % i])\n",
    "            bi_embeddings = torch.mul(ego_embeddings, side_embeddings)\n",
    "            bi_embeddings = torch.matmul(bi_embeddings, self.weight_dict['W_gc_%d' % i])\n",
    "\n",
    "            ego_embeddings = nn.LeakyReLU(negative_slope=0.2)(sum_embeddings + bi_embeddings)\n",
    "            norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
    "            all_embeddings += [norm_embeddings]\n",
    "\n",
    "        all_embeddings = torch.cat(all_embeddings, 1)\n",
    "        u_g_embeddings = all_embeddings[:self.n_user, :]\n",
    "        i_g_embeddings = all_embeddings[self.n_user:, :]\n",
    "\n",
    "        u_g_embeddings = u_g_embeddings[users, :]\n",
    "        pos_i_g_embeddings = i_g_embeddings[pos_items, :]\n",
    "        neg_i_g_embeddings = i_g_embeddings[neg_items, :]\n",
    "\n",
    "        return u_g_embeddings, pos_i_g_embeddings, neg_i_g_embeddings\n",
    "\n",
    "    def BPR_Loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), axis=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), axis=1)\n",
    "\n",
    "        maxi = nn.LogSigmoid()(pos_scores - neg_scores)\n",
    "\n",
    "        mf_loss = -1 * torch.mean(maxi)\n",
    "\n",
    "        regularizer = (torch.norm(users) ** 2 + torch.norm(pos_items) ** 2 + torch.norm(neg_items) ** 2) / 2\n",
    "        decay = 1e-5\n",
    "        emb_loss = decay * regularizer / self.batch_size\n",
    "\n",
    "        return mf_loss + emb_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_v8jmU4t2Ye"
   },
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7CAdhgdxt1-d"
   },
   "outputs": [],
   "source": [
    "def data_load():\n",
    "    global n_users, n_items, n_train, n_test, matrix\n",
    "\n",
    "    train_file = './data/train.txt'\n",
    "    test_file = './data/test.txt'\n",
    "\n",
    "    with open(train_file) as f:\n",
    "        for line in f.readlines():\n",
    "            x = line.strip().split()\n",
    "\n",
    "            user_id = int(x[0])\n",
    "            exist_users.append(user_id)\n",
    "            n_users = max(n_users, user_id)\n",
    "\n",
    "            items = [*map(int, x[1:])]\n",
    "            n_items = max(n_items, max(items))\n",
    "\n",
    "            n_train += len(items)\n",
    "\n",
    "    with open(test_file) as f:\n",
    "        for line in f.readlines():\n",
    "            x = line.strip().split()\n",
    "\n",
    "            items = [*map(int, x[1:])]\n",
    "            n_items = max(n_items, max(items))\n",
    "            n_test += len(items)\n",
    "\n",
    "    n_users += 1\n",
    "    n_items += 1\n",
    "\n",
    "    matrix = torch.zeros((n_users, n_items))\n",
    "\n",
    "    with open(train_file) as f_train:\n",
    "        with open(test_file) as f_test:\n",
    "            for line in f_train.readlines():\n",
    "                x = line.strip().split()\n",
    "                items = [*map(int, x)]\n",
    "                user_id, t_items = items[0], items[1:]\n",
    "                for item in t_items:\n",
    "                    matrix[user_id, item] = 1\n",
    "                train_items[user_id] = t_items\n",
    "            for line in f_test.readlines():\n",
    "                x = line.strip().split()\n",
    "                items = [*map(int, x)]\n",
    "                user_id, t_items = items[0], items[1:]\n",
    "                test_set[user_id] = t_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOZh7BwWuBap"
   },
   "source": [
    "### 학습할 때 필요한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sGqT_VzyuAKS"
   },
   "outputs": [],
   "source": [
    "def sample():\n",
    "    if batch_size <= n_users:\n",
    "        users = random.sample(exist_users, batch_size)\n",
    "    else:\n",
    "        users = [random.choice(exist_users) for i in range(batch_size)]\n",
    "\n",
    "    pos_items, neg_items = [], []\n",
    "    for user in users:\n",
    "        pos_item_list = train_items[user]\n",
    "        pos_batch = pos_item_list[np.random.randint(0, len(pos_item_list))]\n",
    "        pos_items += [pos_batch]\n",
    "\n",
    "        while 1:\n",
    "            neg_item_list = train_items[user]\n",
    "            neg_id = np.random.randint(0, n_items)\n",
    "            if neg_id not in train_items[user] and neg_id not in neg_item_list:\n",
    "                neg_items.append(neg_id)\n",
    "                break\n",
    "    return users, pos_items, neg_items\n",
    "\n",
    "\n",
    "def get_norm_adj():\n",
    "    adj_mat = torch.zeros([n_users + n_items, n_users + n_items])\n",
    "    adj_mat[:n_users, n_users:] = matrix\n",
    "    adj_mat[n_users:, :n_users] = matrix.T\n",
    "    rowsum = np.array(adj_mat.sum(1))\n",
    "    d_inv = rowsum.copy()\n",
    "    for i in range(rowsum.size):\n",
    "        if d_inv[i] != 0:\n",
    "            d_inv[i] = 1 / d_inv[i]\n",
    "    d_mat_inv = np.diag(d_inv)\n",
    "\n",
    "    return torch.from_numpy(d_mat_inv.dot(adj_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxZOHFZuuEcu"
   },
   "source": [
    "### 평가할 때 필요한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wH7jCTGouDLH"
   },
   "outputs": [],
   "source": [
    "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
    "\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "    K_max = Ks\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    if global_epoch_value == total_epoch - 1:\n",
    "        f = open('./data/rank.txt', 'a')\n",
    "        f.write(' '.join(map(str, K_max_item_score)) + '\\n')\n",
    "        f.close()\n",
    "    r = []\n",
    "    for val in K_max_item_score:\n",
    "        if val in user_pos_test:\n",
    "            r += [1]\n",
    "        else:\n",
    "            r += [0]\n",
    "    return r\n",
    "\n",
    "\n",
    "def get_performance(r, Ks):\n",
    "    return np.mean(np.asarray(r)[:Ks])\n",
    "\n",
    "\n",
    "def test_one_user(x, y):\n",
    "    rating = x\n",
    "    user = y\n",
    "    if len(train_items[user]) == 0:\n",
    "        training_items = []\n",
    "    else:\n",
    "        training_items = train_items[user]\n",
    "\n",
    "    user_pos_test = test_set[user]\n",
    "    all_items = set(range(n_items))\n",
    "    test_items = list(all_items - set(training_items))\n",
    "    r = ranklist_by_heapq(user_pos_test, test_items, rating, 1000)\n",
    "\n",
    "    return get_performance(r, 1000)\n",
    "\n",
    "\n",
    "def test(model, users_to_test):\n",
    "    result = 0\n",
    "    u_batch_size = batch_size * 2\n",
    "\n",
    "    test_users = users_to_test\n",
    "    n_test_users = len(test_users)\n",
    "    n_users_batchs = n_test_users // u_batch_size + 1\n",
    "\n",
    "    for u_batch_id in range(n_users_batchs):\n",
    "        start = u_batch_id * u_batch_size\n",
    "        end = (u_batch_id + 1) * u_batch_size\n",
    "\n",
    "        user_batch = test_users[start:end]\n",
    "        item_batch = range(n_items)\n",
    "        u_g_embedding, pos_i_g_embedding, _ = model(user_batch, item_batch, [])\n",
    "        rate_batch = model.rating(u_g_embedding, pos_i_g_embedding).detach().cpu()\n",
    "\n",
    "        for i in range(len(user_batch)):\n",
    "            result += test_one_user(rate_batch.numpy()[i], user_batch[i])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hgs5CZ3EuG3l"
   },
   "source": [
    "### 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zfUo9DFouFre"
   },
   "outputs": [],
   "source": [
    "def get_result(epoch_value = 2000, flag = True):\n",
    "    '''\n",
    "    :param epoch_value: epoch 값\n",
    "    :param flag: True면 이미 저장된 값을 return, False면 새로 학습시켜서 return\n",
    "    :return: 각 유저에게 유사도가 제일 높은 순서로 문제 번호를 반환하는 2차원 리스트\n",
    "    '''\n",
    "    global n_users, n_items, n_train, n_test, total_epoch, embed_size, batch_size, global_epoch_value\n",
    "\n",
    "    result_arr = []\n",
    "\n",
    "    if flag:\n",
    "        if os.path.isfile('./data/rank.txt'):\n",
    "            print('file exist')\n",
    "            f = open('./data/rank.txt', 'r')\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                result_arr.append([*line.strip().split()])\n",
    "            return result_arr\n",
    "        else:\n",
    "            print('file does not exist')\n",
    "\n",
    "    n_users, n_items, n_train, n_test = 0, 0, 0, 0\n",
    "    data_load()\n",
    "    norm_adj = get_norm_adj()\n",
    "    total_epoch = epoch_value\n",
    "    embed_size = 64\n",
    "    batch_size = 1024\n",
    "    layer_size = [64, 64, 64]\n",
    "\n",
    "    model = NGCF(n_users, n_items, norm_adj, embed_size, batch_size, layer_size).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(total_epoch):\n",
    "        global_epoch_value = epoch\n",
    "        time1 = time.time()\n",
    "\n",
    "        loss = 0\n",
    "        n_batch = n_train // batch_size + 1\n",
    "        for idx in range(n_batch):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            users, pos_items, neg_items = sample()\n",
    "            u_g_embedding, pos_i_g_embedding, neg_i_g_embedding = model(users, pos_items, neg_items)\n",
    "            batch_loss = model.BPR_Loss(u_g_embedding, pos_i_g_embedding, neg_i_g_embedding)\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss += batch_loss\n",
    "\n",
    "        time2 = time.time()\n",
    "        print(f'Epoch: {epoch}, loss: {loss}, time: {int(time2 - time1)}')\n",
    "\n",
    "        users_to_test = list(test_set.keys())\n",
    "        ret = test(model, users_to_test)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Precision: {ret}')\n",
    "    \n",
    "    f = open('./data/rank.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        result_arr.append([*line.strip().split()])\n",
    "    return result_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vhR4mD1uJx1"
   },
   "source": [
    "### 학습된 결과 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5d2gZ8QuIgD",
    "outputId": "62f24a5b-9de2-4d8c-8eb4-a6080055f4af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file does not exist\n",
      "Epoch: 0, loss: 275.05853271484375, time: 19\n",
      "Epoch: 1, loss: 148.19906616210938, time: 20\n",
      "Epoch: 2, loss: 138.62149047851562, time: 19\n",
      "Epoch: 3, loss: 134.75172424316406, time: 19\n",
      "Epoch: 4, loss: 130.48680114746094, time: 19\n",
      "Epoch: 5, loss: 126.77500915527344, time: 19\n",
      "Epoch: 6, loss: 122.59662628173828, time: 19\n",
      "Epoch: 7, loss: 118.75921630859375, time: 19\n",
      "Epoch: 8, loss: 116.51404571533203, time: 19\n",
      "Epoch: 9, loss: 114.49104309082031, time: 21\n",
      "Precision: 149.9890000000003\n",
      "Epoch: 10, loss: 113.48184967041016, time: 19\n",
      "Epoch: 11, loss: 111.90494537353516, time: 19\n",
      "Epoch: 12, loss: 110.71029663085938, time: 19\n",
      "Epoch: 13, loss: 108.91736602783203, time: 19\n",
      "Epoch: 14, loss: 107.80640411376953, time: 20\n",
      "Epoch: 15, loss: 107.19461822509766, time: 20\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "get_result(epoch_value = 200, flag = True)\n",
    "epoch_value = epoch 설정\n",
    "flag = 이미 저장된 파일 불러오려면 True, 새로 학습하려면 False\n",
    "'''\n",
    "\n",
    "user_problem_list = get_result(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTHiXcbJ10dJ"
   },
   "source": [
    "## 결과 출력\n",
    "\n",
    "1. 유저가 이미 풀은 문제는 제외\n",
    "2. 유저 id와 문제 id가 매핑되어 있으므로 원래대로 바꾸어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbOCKqMDuTYc"
   },
   "outputs": [],
   "source": [
    "def except_solved_problem(recommender_list):\n",
    "    problem_title_level_df = pd.read_csv(\"./data/userId_problemId.csv\").loc[:, ['problemId', 'title', 'level']]\n",
    "    user_problem_remap_df = pd.read_csv(\"./data/userId_problemId_remap.csv\").loc[:, ['userId', 'problemId']]\n",
    "    user_df = pd.read_csv(\"./data/user_list.csv\")\n",
    "    problem_df = pd.read_csv(\"./data/problem_list.csv\")\n",
    "    \n",
    "    print(user_df.head())\n",
    "\n",
    "    s = input(\"Search user name: \")\n",
    "\n",
    "    try:\n",
    "        user_id = int(user_df[user_df['user_id'] == s]['remap_id'])\n",
    "    except:\n",
    "        print('cannot found user')\n",
    "        return []\n",
    "\n",
    "\n",
    "    user_recommender_list = recommender_list[user_id]\n",
    "    search_user_df = user_problem_remap_df[user_problem_remap_df['userId'] == user_id]\n",
    "    \n",
    "    # remap_id → boj 문제 번호\n",
    "    boj_problem_list = []\n",
    "    for rec_id in user_recommender_list:\n",
    "        if search_user_df[search_user_df['problemId'] == float(rec_id)].size != 0:\n",
    "            boj_problem_list.append(int(problem_df[problem_df['remap_id'] == int(rec_id)]['problem_id']))\n",
    "    \n",
    "    # boj 문제 번호 -> search level, title\n",
    "\n",
    "    res_boj_info = []\n",
    "    for boj_problem_id in boj_problem_list:\n",
    "        df = problem_title_level_df[problem_title_level_df['problemId'] == boj_problem_id].iloc[0]\n",
    "        if 5 < df['level'] <= 15:\n",
    "            res_boj_info.append([df['problemId'], df['title'], df['level']])\n",
    "\n",
    "\n",
    "    return res_boj_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDewrOLQNAIi"
   },
   "outputs": [],
   "source": [
    "# KeyError\n",
    "res_arr = except_solved_problem(user_problem_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32IV_S7RaCec"
   },
   "outputs": [],
   "source": [
    "res_arr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5rkHmQYaiM8"
   },
   "outputs": [],
   "source": [
    "problem_title_df[problem_title_df['problemId'] == 1000].iloc[0]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
